{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAuthor:\\t\\tHrag Najarian\\nDate:\\t\\tAugust 2025\\nPurpose:\\tAct as a worksheet to analyze data from Chapter 3 of my dissertation which consists of\\n\\t\\t\\t\\tinvestigating what is resulting in the intraseaonal change in the diurnal cycle over\\n\\t\\t\\t\\tthe ocean.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Author:\t\tHrag Najarian\n",
    "Date:\t\tAugust 2025\n",
    "Purpose:\tAct as a worksheet to analyze data from Chapter 3 of my dissertation which consists of\n",
    "\t\t\t\tinvestigating what is resulting in the intraseaonal change in the diurnal cycle over\n",
    "\t\t\t\tthe ocean.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrf import getvar, ALL_TIMES, default_fill\n",
    "import matplotlib as mpl\n",
    "import cartopy.crs as ccrs\n",
    "import glob\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from flox.xarray import xarray_reduce\n",
    "from flox import Aggregation\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from math import cos, asin, sqrt, pi, atan, degrees, tan\n",
    "import scipy\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import label, sum as ndi_sum, distance_transform_edt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label, sum as ndi_sum, distance_transform_edt\n",
    "\n",
    "def compute_coast_distance(da_landmask, min_cluster_size, dx_km):\n",
    "    \"\"\"\n",
    "    Removes small land clusters from a LANDMASK and computes the signed distance from the coast.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    da_landmask : xarray.DataArray\n",
    "        Binary landmask where 1 = land, 0 = ocean.\n",
    "    min_cluster_size : int\n",
    "        Minimum number of connected land points to retain as land.\n",
    "        Multiply the min_cluster_size by dx_km^2 to get island area you are removing\n",
    "    dx_km : float\n",
    "        Grid spacing in kilometers for distance scaling.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    distance_from_coast : xarray.DataArray\n",
    "        Distance from coast with land as negative values and ocean as positive values.\n",
    "    \"\"\"\n",
    "    # Remove small land clusters\n",
    "    matrix = da_landmask.values.copy()\n",
    "    labeled_matrix, num_features = label(matrix)\n",
    "    cluster_sizes = ndi_sum(matrix, labeled_matrix, index=np.arange(1, num_features + 1))\n",
    "\n",
    "    for cluster_num, size in enumerate(cluster_sizes, start=1):\n",
    "        if size < min_cluster_size:\n",
    "            matrix[labeled_matrix == cluster_num] = 0\n",
    "\n",
    "    # Update the landmask with filtered values, not including small clusters\n",
    "    cleaned_landmask = da_landmask.copy(data=matrix)\n",
    "\n",
    "    # Compute signed distance from the coast\n",
    "    land_dist = distance_transform_edt(cleaned_landmask.values) * dx_km\n",
    "    ocean_dist = distance_transform_edt((1 - cleaned_landmask.values)) * dx_km\n",
    "    signed_distance = xr.where(cleaned_landmask == 1, -land_dist, ocean_dist)\n",
    "\n",
    "    # Wrap result in DataArray with appropriate metadata\n",
    "    dist_da = cleaned_landmask.copy(data=signed_distance)\n",
    "    dist_da.name = 'DistFromCoast'\n",
    "    dist_da.attrs['Units'] = 'km'\n",
    "    dist_da.attrs['description'] = 'Negative for land, Positive for ocean'\n",
    "\n",
    "    # Restore original coordinates\n",
    "    dist_da = dist_da.assign_coords(\n",
    "        south_north=('south_north', da_landmask.XLAT[:, 0].values),\n",
    "        west_east=('west_east', da_landmask.XLONG[0, :].values)\n",
    "    )\n",
    "\n",
    "    # Create the bins for the distance from coast for groupby/xarray reduce functions\n",
    "    dist_bins = np.arange(dist_da.values.min()-(dist_da.values.min()%dx_km), (dist_da.values.max()+(dx_km-(dist_da.values.max()%dx_km)))+dx_km, dx_km, dtype=int)\n",
    "    dist_bins = dist_bins[dist_bins!=0]\t# Remove 0 because no distance from coast value is == 0\n",
    "\n",
    "    return dist_da, dist_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_and_average_by_distance(da, dist_bins, dim='distance_from_coast'):\n",
    "    \"\"\"\n",
    "    Groups and averages an xarray object by binned distance from coast.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    da : xarray.DataArray or xarray.Dataset\n",
    "        Input da to be grouped and averaged.\n",
    "    dist_bins : array-like\n",
    "        Bins to group the 'distance_from_coast' coordinate.\n",
    "        Output from function compute_coast_distance\n",
    "    dim : str, optional\n",
    "        The coordinate name to bin and group by. Default is 'distance_from_coast'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    xarray.DataArray or xarray.Dataset\n",
    "        The grouped and averaged da with renamed bin coordinate.\n",
    "    \"\"\"\n",
    "    ## Group by using flox\n",
    "    grouped = xarray_reduce(da, dim, func='nanmean', expected_groups=dist_bins, isbin=[True], fill_value=np.nan)\n",
    "\n",
    "    ## Assign bin left edges as coordinate\n",
    "    grouped = grouped.assign_coords({f'{dim}_bins': pd.arrays.IntervalArray(grouped[f'{dim}_bins'].values).left.values})\n",
    "\n",
    "    ## Rename bin coordinate to original dimension name\n",
    "    return grouped.rename({f'{dim}_bins': dim})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wind_direction(u, v):\n",
    "    \"\"\"\n",
    "    Calculate wind direction from zonal and meridional wind components.\n",
    "    \n",
    "    Parameters:\n",
    "    u (float or array-like): Zonal wind component (positive for eastward, negative for westward).\n",
    "    v (float or array-like): Meridional wind component (positive for northward, negative for southward).\n",
    "    \n",
    "    Returns:\n",
    "    float or ndarray: Wind direction in degrees (meteorological convention).\n",
    "    \"\"\"\n",
    "\t# Calculate wind direction in mathematical convention (counterclockwise from positive x-axis)\n",
    "    # math_direction = np.degrees(np.arctan2(v, u))  # Convert radians to degrees\n",
    "\n",
    "    # Convert to meteorological convention (clockwise from true north)\n",
    "    direction = (180 + np.degrees(np.arctan2(-u, -v))) % 360\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes cartesian coordinate system\n",
    "def calculate_angle_between_points(p1, p2):\n",
    "    # Calculate differences\n",
    "    dy = p2[0] - p1[0]  # Lats\n",
    "    dx = p2[1] - p1[1]  # Lons\n",
    "    # Find the angle (radians)\n",
    "    theta = math.atan(dy/dx)\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# # Example points\n",
    "# point1 = [0, 0]\n",
    "# point2 = [1, 1]\n",
    "\n",
    "# theta = math.degrees(calculate_angle_between_points(point1, point2))\n",
    "# print(f\"The angle between the points is {theta} degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertical_cross_integration(da, p_bot, p_top):\n",
    "\t# Do a rolling mean to get the average between two pressure levels, remove the first level of da since it's nan's, and sel between two P levels\n",
    "\tda = da.rolling(bottom_top=2).mean().isel(bottom_top=slice(1,None)).sel(bottom_top=slice(p_bot,p_top))\n",
    "\t# Create dp and make it the same shape as da.\n",
    "\tdp_levels = da.bottom_top.diff(dim='bottom_top').expand_dims(dim={'Time':len(da.Time),'Distance':len(da.Distance),'Spread':len(da.Spread)})\n",
    "\n",
    "\tda_vertically_integrated = (da * dp_levels).sum('bottom_top') / (-9.81)\n",
    "\n",
    "\treturn da_vertically_integrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5e-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def round_to_two_significant_figures(number):\n",
    "    \"\"\"\n",
    "    Rounds a float value to two significant figures using np.ceil or np.floor.\n",
    "\n",
    "    Args:\n",
    "        number (float): The input float value.\n",
    "\n",
    "    Returns:\n",
    "        float: The rounded value.\n",
    "    \"\"\"\n",
    "    # Calculate the order of magnitude (power of 10) for the input number\n",
    "    order_of_magnitude = np.floor(np.log10(np.abs(number)))\n",
    "\n",
    "    # Calculate the factor to round to two significant figures\n",
    "    factor = 10**(2 - order_of_magnitude)\n",
    "\n",
    "    # Round the number using np.ceil or np.floor\n",
    "    rounded_value = np.ceil(number * factor) / factor\n",
    "\n",
    "    return rounded_value\n",
    "\n",
    "def round_to_one_significant_figures(number):\n",
    "    \"\"\"\n",
    "    Rounds a float value to two significant figures using np.ceil or np.floor.\n",
    "\n",
    "    Args:\n",
    "        number (float): The input float value.\n",
    "\n",
    "    Returns:\n",
    "        float: The rounded value.\n",
    "    \"\"\"\n",
    "    # Calculate the order of magnitude (power of 10) for the input number\n",
    "    order_of_magnitude = np.floor(np.log10(np.abs(number)))\n",
    "\n",
    "    # Calculate the factor to round to two significant figures\n",
    "    factor = 10**(1 - order_of_magnitude)\n",
    "\n",
    "    # Round the number using np.ceil or np.floor\n",
    "    rounded_value = np.ceil(number * factor) / factor\n",
    "\n",
    "    return rounded_value\n",
    "\n",
    "round_to_one_significant_figures(0.00001427)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Function based on the scipy.stats.linregress function\n",
    "\n",
    "# Make sure the x and y values are 1-D matrices\n",
    "\n",
    "# Inclusion of custom start and end lag inputs. \n",
    "\t# If you don't care for the negative lags, then start at 0, or vise versa.\n",
    "\t# This way you're not wasting resources on lags you don't need.\n",
    "\n",
    "def linreg(x, y, min_lag, max_lag):\n",
    "\t# Initialize matrices\n",
    "\tslopes = []\n",
    "\tyintercepts = []\n",
    "\trvalues = []\n",
    "\tpvalues = []\n",
    "\tstderrors = []\n",
    "\t# How to disypher what lag relationships mean:\n",
    "\t\t# Negative lag implies that x at the moment correlates with your y in the future\n",
    "\t\t# Positive lag implies that x at the moment correlates with you y in the past\n",
    "\tfor lag in range(min_lag,max_lag+1):\t# itterate from min_lag to max_lag (+1 because of range function)\n",
    "\t\tif lag == 0:\t\n",
    "\t\t\tslope, yintercept, rvalue, pvalue, stderror = scipy.stats.linregress(x, y)\n",
    "\t\telif lag < 0:\n",
    "\t\t\tslope, yintercept, rvalue, pvalue, stderror = scipy.stats.linregress(x[:lag], y[-lag:])\n",
    "\t\telif lag > 0:\n",
    "\t\t\tslope, yintercept, rvalue, pvalue, stderror = scipy.stats.linregress(x[lag:], y[:-lag])\n",
    "\t\t\n",
    "\t\t# Append the values!\n",
    "\t\tslopes.append(slope)\n",
    "\t\tyintercepts.append(yintercept)\n",
    "\t\trvalues.append(rvalue)\n",
    "\t\tpvalues.append(pvalue)\n",
    "\t\tstderrors.append(stderror)\n",
    "\n",
    "\t# Compile data into a dataarray for easy management\n",
    "\tda_reg = xr.DataArray(\n",
    "\t\tdata=np.arange(min_lag,max_lag+1),\n",
    "\t\tdims='lag',\n",
    "\t\tcoords=dict(\n",
    "\t\t\tslope = ('lag', slopes),\n",
    "\t\t\tyintercept = ('lag', yintercepts),\n",
    "\t\t\trvalue = ('lag', rvalues),\n",
    "\t\t\tpvalue = ('lag', pvalues),\n",
    "\t\t\tstderror= ('lag',stderrors)\n",
    "\t\t\t),\n",
    "\t\tname='lin_reg'\n",
    "\t\t)\n",
    "\n",
    "\treturn da_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Temperature [K] via potential temperature equation [PT = T * (Po/P) ^ .286]\n",
    "\t# Link: https://glossary.ametsoc.org/wiki/Potential_temperature \n",
    "def theta_to_temp(theta):\n",
    "\t# PT = T * (P0/P)^(R/Cp)\n",
    "\t\t# PT = Potential temperature/theta\n",
    "\t\t# T = Temperature\n",
    "\t\t# P0 = 1000 hPa\n",
    "\t\t# P = Pressure\n",
    "\t\t# R = Gas constant for air (287.052874 J/(kg*K))\n",
    "\t\t# Cp = Specific heat capacity at constant pressure (1003.5 J/(kg*K))\n",
    "\t\t\t# R/Cp = 0.286\n",
    "\t# so\n",
    "\t# T = PT / (P0/P)^(0.286)\n",
    "\ttemp = xr.zeros_like(theta)\n",
    "\tP = theta.bottom_top.values\n",
    "\tfor i in range(len(P)):\n",
    "\t\ttemp[:,i,...] = theta[:,i,...] / (1000/P[i])**(0.286)\n",
    "\t\n",
    "\ttemp.name = 'Temperature'\n",
    "\ttemp = temp.assign_attrs(Units='K')\n",
    "\t\n",
    "\treturn temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Potential Temperature [K] via potential temperature equation [PT = T * (Po/P) ^ .286]\n",
    "def temp_to_theta(temp, psfc):\n",
    "\t# PT = T * (P0/P)^(R/Cp)\n",
    "\t\t# PT = Potential temperature/theta\n",
    "\t\t# T = Temperature\n",
    "\t\t# P0 = 1000 hPa\n",
    "\t\t# P = Pressure\n",
    "\t\t# R = Gas constant for air (287.052874 J/(kg*K))\n",
    "\t\t# Cp = Specific heat capacity at constant pressure (1003.5 J/(kg*K))\n",
    "\t\t\t# R/Cp = 0.286\n",
    "\t\n",
    "\ttheta = xr.zeros_like(temp)\n",
    "\ttheta = temp* ((1000/psfc)**(0.286))\n",
    "\n",
    "\ttheta.name = 'Potential Temperature'\n",
    "\ttheta = theta.assign_attrs(Units='K')\n",
    "\n",
    "\treturn theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mixing Ratio [kg/kg]\n",
    "def mixing_ratio(e):\n",
    "\t# Calculate mixing ratio\n",
    "\t# https://glossary.ametsoc.org/wiki/Mixing_ratio\n",
    "\tr = xr.zeros_like(e)\n",
    "\tP = e.bottom_top.values\n",
    "\tfor i in range(len(P)):\n",
    "\t\tr[:,i,...] = 0.62197*((e[:,i,...])/(P[i]-e[:,i,...]))\t# [kg/kg]\n",
    "\t\n",
    "\tr.name = 'Mixing Ratio'\n",
    "\tr = r.assign_attrs(Units='kg/kg')\n",
    "\t\n",
    "\treturn r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Virtual Temperature [K] via Tv = T(1+(rv/eps)) / (1+rv)\n",
    "def virt_temp(T, rv, eps):\n",
    "\t# Calculate virtual temperature\n",
    "\t# https://glossary.ametsoc.org/wiki/Virtual_temperature\n",
    "\tTv = xr.zeros_like(T)\n",
    "\tTv = T*(1+(rv/eps)) / (1+rv)\n",
    "\n",
    "\tTv.name = 'Virtual Temperature'\n",
    "\tTv = Tv.assign_attrs(Units='K')\n",
    "\n",
    "\treturn Tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Saturation Vapor Pressure \n",
    "def sat_vap_press(temperature):\n",
    "\t# Follow the Clausius-Clapeyron equation \n",
    "\t\t# Link: https://geo.libretexts.org/Bookshelves/Meteorology_and_Climate_Science/Practical_Meteorology_(Stull)/04%3A_Water_Vapor/4.00%3A_Vapor_Pressure_at_Saturation\n",
    "\t\t# es = e0 * exp([L/Rv] * (1/T0 - 1/T))\n",
    "\te0=6.113\t\t# [hPa]\n",
    "\tRv=461\t\t\t# [J/(K*kg)]\n",
    "\tT0=273.15\t\t# [K]\n",
    "\tLv=2.5*(10**6)\t# [J/kg]\t# liquid water\n",
    "\tLd=2.83*(10**6)\t# [J/kg]\t# ice water\n",
    "\t# Create matrix/dataarray da that has variable L values based on the \n",
    "\t\t# temperature of the atmosphere where it changes based on if T < 0°C \n",
    "\t\t# Divide by rv to create the constant L/Rv\n",
    "\tconstant = temperature.where(temperature>273.15, Lv)/Rv\n",
    "\tconstant = temperature.where(temperature<=273.15, Ld)/Rv\n",
    "\n",
    "\tes = e0 * np.exp(constant * ((1/T0)-(1/temperature)))\n",
    "\n",
    "\tes.name = 'Saturation Vapor Pressure'\n",
    "\tes = es.assign_attrs(Units='hPa')\n",
    "\t\n",
    "\treturn es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sat_mixing_ratio(es, psfc):\n",
    "\t# Calculate saturated mixing ratio\n",
    "\t# https://glossary.ametsoc.org/wiki/Mixing_ratio\n",
    "\tws = xr.zeros_like(es)\n",
    "\tif 'bottom_top' in es.dims:\n",
    "\t\tP = es.bottom_top.values\n",
    "\t\tfor i in range(len(P)):\n",
    "\t\t\tws[:,i,...] = 0.62197*((es[:,i,...])/(P[i]-es[:,i,...]))\t# [kg/kg]\n",
    "\telse:\n",
    "\t\tP = psfc\n",
    "\t\tws = 0.62197*((es)/(P-es))\t# [kg/kg]\n",
    "\t\n",
    "\tws.name = 'Saturation Mixing Ratio'\n",
    "\tws = ws.assign_attrs(Units='kg/kg')\n",
    "\t\n",
    "\treturn ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_hum_theta(theta,w):\n",
    "\t# Calculate the relative humidity via observed potential temperature [K] and mixing ratio [kg/kg]\n",
    "\ttemperature = theta_to_temp(theta)\t# [K]\n",
    "\tes = sat_vap_press(temperature)\t\t# [hPa]\n",
    "\tws = sat_mixing_ratio(es,es.bottom_top.values)\t# [kg/kg]\n",
    "\tRH = w/ws * 100\t\t\t\t\t\t# [%]\n",
    "\n",
    "\tRH.name = 'Relative Humidity'\n",
    "\tRH = RH.assign_attrs(Units='%')\n",
    "\n",
    "\treturn RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_hum_temp(temp,w,psfc):\n",
    "\t# Calculate the relative humidity via observed temperature [K] and mixing ratio [kg/kg]\n",
    "\ttemperature = temp\t\t\t\t\t# [K]\n",
    "\tes = sat_vap_press(temperature)\t\t# [hPa]\n",
    "\tws = sat_mixing_ratio(es,psfc)\t\t# [kg/kg]\n",
    "\tRH = w/ws * 100\t\t\t\t\t\t# [%]\n",
    "\n",
    "\tRH.name = 'Relative Humidity'\n",
    "\tRH = RH.assign_attrs(Units='%')\n",
    "\n",
    "\treturn RH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: To grab the indicies that correspond to the times, latitudes, and longitudes of the WRF dataset file.\n",
    "\n",
    "# Input:\n",
    "\t# file == path to the .nc file\n",
    "\t# times == np.datetime64 array [Start,End]\n",
    "\t# lats == np.array [south,north]\n",
    "\t# lons == np.array [west,east]\n",
    "\n",
    "# Output:\n",
    "\t# time_ind, lat_ind, lon_ind == corresponds to the indicies of the times, lats, and lons provided within that file\n",
    "\n",
    "# Example:\n",
    "\t# file = '/ourdisk/hpc/radclouds/auto_archive_notyet/tape_2copies/hragnajarian/wrfout.files/10day-2015-11-22-12--12-03-00/raw/d01'\n",
    "\t# times = [np.datetime64('2015-11-22T12'), np.datetime64('2015-11-23T12')]\n",
    "\t# lats = [-7.5, 7.5]\n",
    "\t# lons = [90, 110]\n",
    "\t# isel_ind(file, times, lats, lons)\n",
    "\n",
    "def isel_ind(file,times,lats,lons):\n",
    "\t# Declare the variables\n",
    "\ttime_ind = np.zeros(2, dtype=int)\n",
    "\tlat_ind = np.zeros(2, dtype=int)\n",
    "\tlon_ind = np.zeros(2, dtype=int)\n",
    "\t# Open the file\n",
    "\tds = xr.open_dataset(file)\n",
    "\t# Times\n",
    "\ttime_vals = ds.XTIME.compute().values\n",
    "\ttime_mask = (time_vals >= times[0]) & (time_vals <= times[1])\n",
    "\ttime_indices = np.where(time_mask)[0]\n",
    "\ttime_ind = [time_indices[0], time_indices[-1] + 1]  # +1 for Python slicing to include last index\n",
    "\t# Latitudes\n",
    "\tlat_vals = ds.XLAT[0,:,0].compute().values\n",
    "\tlat_mask = (lat_vals >= lats[0]) & (lat_vals <= lats[1])\n",
    "\tlat_indices = np.where(lat_mask)[0]\n",
    "\tlat_ind = [lat_indices[0], lat_indices[-1] + 1]  # +1 for Python slicing to include last index\n",
    "\t# Longitude\n",
    "\tlon_vals = ds.XLONG[0,0,:].compute().values\n",
    "\tlon_mask = (lon_vals >= lons[0]) & (lon_vals <= lons[1])\n",
    "\tlon_indices = np.where(lon_mask)[0]\n",
    "\tlon_ind = [lon_indices[0], lon_indices[-1] + 1]  # +1 for Python slicing to include last index\n",
    "\n",
    "\n",
    "\treturn time_ind, lat_ind, lon_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Opens a dataset with restrictive bounds to make opening large files less intensive\n",
    "\n",
    "# Input:\n",
    "\t# file == path to the .nc file\n",
    "\t# time_ind == indicies (inclusive) of the dates you want to look at\n",
    "\t# lat_ind == indicies (inclusive) of the latitudes you want to look at\n",
    "\t# lon_ind == indicies (inclusive) of the longitudes you want to look at\n",
    "\n",
    "# Output:\n",
    "\t# ds == dataset that corresponds to the times, lats, and lons provided.\n",
    "\n",
    "# Example:\n",
    "\t# To get the indicies, I suggest using the function isel_ind() I have coded up in tandem with this function (see above).\n",
    "\t# file = '/ourdisk/hpc/radclouds/auto_archive_notyet/tape_2copies/hragnajarian/wrfout.files/10day-2015-11-22-12--12-03-00/raw/d01'\n",
    "\t# times = [np.datetime64('2015-11-22T12'), np.datetime64('2015-11-23T12')]\n",
    "\t# lats = [-7.5, 7.5]\n",
    "\t# lons = [90, 110]\n",
    "\t# time_ind, lat_ind, lon_ind = isel_ind(file, times, lats, lons)\n",
    "\t# ds = open_ds(file,time_ind,lat_ind,lon_ind)\n",
    "\n",
    "def open_ds(file, time_ind, lat_ind, lon_ind):\n",
    "\n",
    "\tif time_ind[-1] == -1:\n",
    "\t\tds = xr.open_dataset(file, chunks='auto').isel(\n",
    "\t\tTime=slice(None),\n",
    "\t\t# south_north=slice(lat_ind[0],lat_ind[1]),\n",
    "\t\t# west_east=slice(lon_ind[0],lon_ind[1])\n",
    "\t\tsouth_north=slice(None),\n",
    "\t\twest_east=slice(None)\n",
    "\t)\n",
    "\telse:\n",
    "\t\tds = xr.open_dataset(file, chunks='auto').isel(\n",
    "\t\t\tTime=slice(time_ind[0],time_ind[1]),\n",
    "\t\t\tsouth_north=slice(lat_ind[0],lat_ind[1]),\n",
    "\t\t\twest_east=slice(lon_ind[0],lon_ind[1])\n",
    "\t\t)\n",
    "\t\n",
    "\treturn ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the distance [km] between two coordinates in lat & lon. \n",
    "    # If your map projection is Mercator (check 'MAP_PROJ'), then this works. \n",
    "    # If Lambert, then you need to do conversions, look at ChatGPT logs ('Distance Calculation Approximation')\n",
    "def dist(lat1, lon1, lat2, lon2):\n",
    "    r = 6371 # km\n",
    "    p = pi / 180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return 2 * r * asin(sqrt(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Rotate a vectorized variable like a wind vector in x/lon and y/lat space!\n",
    "\t# See Lecture02_Vector.pdf in METR 5113 Advanced Atmospheric Dynamics folder\n",
    "\n",
    "# Input:\n",
    "    # da_x  = \t\txr.data_array\t\t \t# Should be the X component of the vector\n",
    "    # da_y  = \t\txr.data_array\t\t\t# Should be the Y component of the vector\n",
    "\t\t# Can be one time step or pressure level or even a 4-D variable [Time x Height x Lat x Lon]\n",
    "    # theta =\t\trotation in radians\t\t\n",
    "\t\t# Must be -pi/2 < theta < pi/2, you don't need to rotate any more than the 180° provided\n",
    "\t\t# Positive theta indiates anticlockwise turn of coordinates\n",
    "\t\t# Negative theta indiates clockwise turn of coordinates\n",
    "# Output:\n",
    "    # da_x_rot:\t\tda_x but rotated\n",
    "\t# da_y_rot:\t\tda_y but rotated\n",
    "# Process:\n",
    "    # Check if theta is positive or negative\n",
    "\t\t# Based on the value, the projections using theta will change\n",
    "\t# Project the current x and y coordinate to the new rotated x and y coordinates for each component.\n",
    "\t# Once rotated, return the rotated x and y components\n",
    "\n",
    "def rotate_vec(da_x, da_y, theta):\n",
    "\t# anti-clockwise rotation\n",
    "\tif (theta > 0) & (theta<pi/2):\n",
    "\t\tda_x_rot = da_x*cos(theta) + da_y*cos((pi/2)-theta)\n",
    "\t\tda_y_rot = -da_x*cos((pi/2)-theta) + da_y*cos(theta)\n",
    "\t# clockwise rotation\n",
    "\tif (theta < 0) & (theta>(-pi/2)):\n",
    "\t\tda_x_rot = da_x*cos(-theta) - da_y*cos((pi/2)+theta)\n",
    "\t\tda_y_rot = da_x*cos((pi/2)+theta) + da_y*cos(-theta)\n",
    "\n",
    "\treturn da_x_rot, da_y_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Create an array with multiple cross-sectional data from WRFoutput.\n",
    "# Input:\n",
    "    # da = \t\t\txr.data_array\t\t \tworks with both 2-D and 3-D variables!\n",
    "    # start_coord = [latitude, longitude] \n",
    "    # end_coord = \t[latitude, longitude]\n",
    "\t# width = \t\tspread of cross-section in degrees i.e., 0.75° = 0.75\n",
    "\t# dx = \t\t\tdistance between each cross-sectional line i.e., 0.05° = 0.05\n",
    "# Output:\n",
    "    # da_cross: \t\tmatrix in time, height, distance, and # of lines\n",
    "\t\t# \t\t\t\t\tor time, distance, and # of lines if using a 2-D variable\n",
    "\t# all_line_coords:\t\n",
    "# Process:\n",
    "\t# Make sure you 'da' have assigned coordinates corresponding to south_north and west_east.\n",
    "    # We first create a main line between start_coord -> end_coord. This line will be the center\n",
    "\t\t# line for all other lines to sit next to.\n",
    "\t# Depending on the angle of the line (more latitudinal change or longitudinal change), this\n",
    "\t\t# function will account for that and make the longest side the length of the cross-sectional line\n",
    "\t# We then create an empty np.array, calculate the change in spread needed, then start filling in the data.\n",
    "\n",
    "def cross_section_multi(da, start_coord, end_coord, width, dx):\n",
    "\n",
    "\t# We want to first create a line between start and end_coords\n",
    "\t\t# Gather the indicies of the closest gridboxes of start and end_coords.\n",
    "\tstart_ilat = int((abs((da.XLAT[0,:,0]) - (start_coord[0]))).argmin())\n",
    "\tend_ilat = int((abs((da.XLAT[0,:,0]) - (end_coord[0]))).argmin())\n",
    "\tstart_ilon = int((abs((da.XLONG[0,0,:]) - (start_coord[1]))).argmin())\n",
    "\tend_ilon = int((abs((da.XLONG[0,0,:]) - (end_coord[1]))).argmin())\n",
    "\t# This statement ensures that the length of the line will be the size of the LONGEST side (either lat or lon)\n",
    "\tif abs(start_ilat-end_ilat)>=abs(start_ilon-end_ilon):\n",
    "\t\tline_coords = np.zeros([2,abs(start_ilat-end_ilat)])\n",
    "\t\t# Populate latitudes\n",
    "\t\tline_coords[0,:] = np.linspace(da.XLAT[0,start_ilat,0], da.XLAT[0,end_ilat,0], abs(start_ilat-end_ilat))\n",
    "\t\t# Populate longitudes\n",
    "\t\tline_coords[1,:] = np.linspace(da.XLONG[0,0,start_ilon], da.XLONG[0,0,end_ilon], abs(start_ilat-end_ilat))\n",
    "\telse:\n",
    "\t\tline_coords = np.zeros([2,abs(start_ilon-end_ilon)])\n",
    "\t\t# Populate latitudes\n",
    "\t\tline_coords[0,:] = np.linspace(da.XLAT[0,start_ilat,0], da.XLAT[0,end_ilat,0], abs(start_ilon-end_ilon))\n",
    "\t\t# Populate longitudes\n",
    "\t\tline_coords[1,:] = np.linspace(da.XLONG[0,0,start_ilon], da.XLONG[0,0,end_ilon], abs(start_ilon-end_ilon))\n",
    "\n",
    "\t##########################################################################################################################\n",
    "\t# Now that we have the coordinates between the start and end_coords, we need to replicate it for all the lines\n",
    "\tnum_lines = int(width/dx)\n",
    "\tspread = np.arange(width/2,-width/2,-dx)\n",
    "\tif 'bottom_top' in da.dims:\t# If the dataset is 3-D\n",
    "\t\t#\t\t\t\t\t\tTIME\tx\tHEIGHT\tx\t   DISTANCE\t   x   #ofLINES\n",
    "\t\tda_cross = np.zeros([da.shape[0],da.shape[1],line_coords.shape[1],num_lines])\n",
    "\telse:\n",
    "\t\t#\t\t\t\t\t\tTIME\tx\t   DISTANCE\t   x   #ofLINES\n",
    "\t\tda_cross = np.zeros([da.shape[0],line_coords.shape[1],num_lines])\n",
    "\n",
    "\t# Create all_line_coords that holds all the coordinates for every line produced\n",
    "\tall_line_coords = np.zeros([line_coords.shape[0],line_coords.shape[1],spread.shape[0]])\n",
    "\n",
    "\t# Looping over all the lines\n",
    "\tfor i in range(len(spread)):\n",
    "\t\t\n",
    "\t\t# Fix this part\n",
    "\t\tif (end_coord[0] > start_coord[0]):\n",
    "\t\t\tall_line_coords[0,:,i] = line_coords[0,:]+spread[i]\n",
    "\t\telse:\n",
    "\t\t\tall_line_coords[0,:,i] = line_coords[0,:]-spread[i]\n",
    "\t\tif (end_coord[1] > start_coord[1]):\n",
    "\t\t\tall_line_coords[1,:,i] = line_coords[1,:]-spread[i]\n",
    "\t\telse:\n",
    "\t\t\tall_line_coords[1,:,i] = line_coords[1,:]+spread[i]\n",
    "\n",
    "\t\t# Now that we have our lines, we can interpolate the dataset with the offset for each line applied\n",
    "\t\tda_interp = da.interp(south_north=all_line_coords[0,:,i], west_east=all_line_coords[1,:,i], method=\"linear\")\n",
    "\n",
    "\t\t# Populate the new data array with data from the cross section\n",
    "\t\t\t# Loop through the length of the line, find the match, and then populate it.\n",
    "\t\tif 'bottom_top' in da.dims:\t# If the dataset is 3-D\n",
    "\t\t\tfor j in range(da_cross.shape[2]):\n",
    "\t\t\t\tdata = da_interp.sel(\n",
    "\t\t\t\t\tsouth_north = da_interp.south_north[j],\n",
    "\t\t\t\t\twest_east = da_interp.west_east[j])\n",
    "\t\t\t\tda_cross[:,:,j,i] = data\n",
    "\t\telse:\n",
    "\t\t\tfor j in range(da_cross.shape[1]):\n",
    "\t\t\t\tdata = da_interp.sel(\n",
    "\t\t\t\t\tsouth_north = da_interp.south_north[j],\n",
    "\t\t\t\t\twest_east = da_interp.west_east[j])\n",
    "\t\t\t\tda_cross[:,j,i] = data\n",
    "\n",
    "\treturn da_cross, all_line_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Create a LocalTime coordinate within your DataArray.\n",
    "\n",
    "# Input:\n",
    "    # da = xr.DataArray;  Must be in shape: time x south_north x west_east\n",
    "\t\t# Make sure west_east/XLONG values are 0->360, not -180->+180\n",
    "\t# dim_num = 2 or 3;  This indicates to the function if you want Local Time\n",
    "\t\t# within the dataarray to be only a function of time and longitude, or\n",
    "\t\t# time, lognitude, and latitude. This is a preference and if you don't need\n",
    "\t\t# it as a function of latitude, it will save lots of time going with dim_num = 2.\n",
    "# Output:\n",
    "    # da: This will be the DataArray with the newly assigned coordinate\n",
    "# Process:\n",
    "    # First create a matrix of hours to be offset relative to UTC.\n",
    "    # Create an empty array that has dimensions Time and Longitude.\n",
    "    # Loop through each timestep and longitude to determine the local time.\n",
    "    # Assign the new Local Time coordinate to the da and return it.\n",
    "\n",
    "\n",
    "def assign_LT_coord(da, dim_num):\n",
    "\thour_offset = (da.XLONG.values[:,0,:]/15).round(decimals=0)\n",
    "\n",
    "\t# Local Time is a function of only Time and Longitude\n",
    "\tif dim_num==2:\n",
    "\t\tlocal_time = np.empty([len(da.Time),len(da.west_east)], dtype=object)\n",
    "\t\tfor i in range(local_time.shape[0]):\n",
    "\t\t\tfor j in range(local_time.shape[1]):\n",
    "\t\t\t\tlocal_time[i,j] = da.Time.values[i] + np.timedelta64(int(hour_offset[0,j]),'h')\n",
    "\t\tda = da.assign_coords(LocalTime=(('Time','west_east'),local_time))\n",
    "\t\n",
    "\t# Local Time is a function of Time, Longitude, and Latitude\n",
    "\telse:\n",
    "\t\tlocal_time = np.empty([len(da.Time),len(da.south_north),len(da.west_east)], dtype='datetime64[ns]')\n",
    "\t\tfor i in range(local_time.shape[0]):\n",
    "\t\t\tfor j in range(local_time.shape[2]):\n",
    "\t\t\t\tlocal_time[i,:,j] = da.Time.values[i] + np.timedelta64(int(hour_offset[0,j]),'h')\n",
    "\t\tda = da.assign_coords(LocalTime=(('Time','south_north','west_east'),local_time))\n",
    "\treturn da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes cartesian/flat coordinate system\n",
    "def calculate_angle_between_points(p1, p2):\n",
    "    # Calculate differences\n",
    "    dy = p2[0] - p1[0]  # Lats\n",
    "    dx = p2[1] - p1[1]  # Lons\n",
    "    # Find the angle (radians)\n",
    "    theta = atan(dy/dx)\n",
    "\n",
    "    # Negative if NW or SE direction\n",
    "    # Positive if NE or SW direction\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# start_coord\t\t= [1.2,112.8]\n",
    "# end_coord \t\t= [5.2,108.8]\n",
    "# degrees(calculate_angle_between_points(start_coord, end_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes making tick labels easier\n",
    "# ticks are integers\n",
    "def degree_labels(ticks, lat_or_lon):\n",
    "\n",
    "\tlabels = []\n",
    "\t\n",
    "\tif lat_or_lon=='lat':\n",
    "\t\tfor t in ticks:\n",
    "\t\t\tif t >= 0:\n",
    "\t\t\t\tlabels.append(str(t)+u'\\N{DEGREE SIGN}N')\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabels.append(str(t)+u'\\N{DEGREE SIGN}S')\n",
    "\t\t\t\t\n",
    "\telif lat_or_lon=='lon':\n",
    "\t\tfor t in ticks:\n",
    "\t\t\tif t >= 0:\n",
    "\t\t\t\tlabels.append(str(t)+u'\\N{DEGREE SIGN}E')\n",
    "\t\t\telse:\n",
    "\t\t\t\tlabels.append(str(t)+u'\\N{DEGREE SIGN}W')\n",
    "\treturn labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_path(parent_dir: str, sub_dir: str, name: str) -> str:\n",
    "    return f\"{parent_dir}/{sub_dir}/{name}\"\n",
    "\n",
    "def load_variable(file_dict: dict, coords: dict, is_diff: bool) -> xr.DataArray:\n",
    "    step_start = time.perf_counter()\n",
    "    # If its a difference of already sliced datasets, don't slice further\n",
    "    if is_diff:\n",
    "        # ds = open_ds(file_dict['path'], [0,-1], lat_ind_d02, lon_ind_d02)\n",
    "        ds = open_ds(file_dict['path'], [0,-1], [0,-1], [0,-1])\n",
    "    else:\n",
    "        ds = open_ds(file_dict['path'], time_ind_d02, lat_ind_d02, lon_ind_d02)\n",
    "        \n",
    "    da = ds[file_dict['varname']].compute()\n",
    "    da = da.assign_coords(coords)\n",
    "    # Replace fill value with nan\n",
    "    da = da.where(da != default_fill(np.float32))\n",
    "    print(f\"{file_dict['description']} loaded ✓\", time.perf_counter() - step_start, \"seconds\")\n",
    "    return da\n",
    "\n",
    "# Function that can removes the bottom_top dimension for 2-D datasets\n",
    "def without_keys(d, keys):\n",
    "\treturn {x: d[x] for x in d if x not in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_load_or_save(file_diff:dict, files_1:dict, files_2:dict, coords:dict, file_1_and_2_diff:bool):\n",
    "\t\n",
    "\t\"\"\"\n",
    "    Load a difference DataArray from disk if it exists; otherwise, compute the difference from two input files,\n",
    "    remove coordinates to reduce file size, save it, and return the result with desired coordinates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_diff : dict\n",
    "        Dictionary with keys including 'path', 'varname', and 'description' for the output difference file.\n",
    "    files_1 : dict\n",
    "        Dictionary containing metadata and path info for the first input file.\n",
    "    files_2 : dict\n",
    "        Dictionary containing metadata and path info for the second input file.\n",
    "    coords : dict\n",
    "        Dictionary of coordinates to assign to the returned DataArray.\n",
    "    file_1_and_2_diff : bool\n",
    "        If True, treats `files_1` and `files_2` as already pre-computed differences (disables additional slicing).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        The difference DataArray WITH coordinates assigned but not saved in the output file.\n",
    "    \"\"\"\n",
    "\n",
    "\tdirectory = os.path.dirname(file_diff['path'])\n",
    "\tfiles = [os.path.join(directory, f) for f in os.listdir(directory)]\n",
    "\t\n",
    "\t# If the file exists, just load it up\n",
    "\tif file_diff['path'] in files:\n",
    "\t\tda_diff = load_variable(file_diff, coords, is_diff=True)\n",
    "\t# Compute it and save it if it doesn't exist\n",
    "\telse:\n",
    "\t\tif file_1_and_2_diff:\t# If files 1 and 2 are already differences, don't slice\n",
    "\t\t\tda_1 = load_variable(files_1, coords, is_diff=True)\n",
    "\t\t\tda_2 = load_variable(files_2, coords, is_diff=True)\n",
    "\t\telse:\n",
    "\t\t\tda_1 = load_variable(files_1, coords, is_diff=False)\n",
    "\t\t\tda_2 = load_variable(files_2, coords, is_diff=False)\n",
    "\t\t\n",
    "\t\tstep_start = time.perf_counter()\n",
    "\t\t# Take difference\n",
    "\t\tda_diff = da_1 - da_2\n",
    "\t\t# Update variable name and description\n",
    "\t\tda_diff.name = file_diff['varname']\n",
    "\t\tda_diff.attrs['description'] = file_diff['description']\n",
    "\n",
    "\t\t# Create a new DataArray with the same data and dims, but no coords (saves 3-times the storage space + load times)\n",
    "\t\tda_diff_stripped = xr.DataArray(\n",
    "\t\t\tdata=da_diff.data,\n",
    "\t\t\tdims=da_diff.dims,\n",
    "\t\t\tname=da_diff.name,\n",
    "\t\t\tattrs=da_diff.attrs\n",
    "\t\t)\n",
    "\n",
    "\t\t# Save File\n",
    "\t\tda_diff_stripped.to_netcdf(path=file_diff['path'], mode='w', format='NETCDF4', unlimited_dims='Time', compute=True)\n",
    "\t\t\n",
    "\t\tprint(f\"{file_diff['description']} loaded ✓\", time.perf_counter() - step_start, \"seconds\")\n",
    "\n",
    "\treturn da_diff.assign_coords(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control Data\n",
    "This section opens stitched wrfout .nc files. The raw .nc file is much to large, so I have extracted variables from the large file into smaller .nc files that only contains one variable. To get these smaller .nc files, please refer to extract_variable.py and interp_variable.py function I have created. This section opens those files up and assigned them to a variable.\n",
    "\n",
    "* 10.5-day (2015 11-22-12UTC -> 2015 12-03-00UTC)\n",
    "* 10.5-day (2015 12-09-12UTC -> 2015 12-20-00UTC)\n",
    "\n",
    "Below the section where I assigned file names, there is an option for you to only open up a smaller domain (spatial and temporal) if you are interetsed in a select region (helpful for debugging). This will save you a lot of computational time.\n",
    "\n",
    "__Adjusted LH__\n",
    "\n",
    "Adjusted LH (Surface Latent Heat Release) is an experiment conducted on the CTRL simulation that adjusts the LH in WRF to be similar to the LH during the timeperiod of the opposite MJO phase of the CTRL. For example, if the CTRL time period is during the PreMJO, the adjLH simulation will take place in the PreMJO time period with the LH adjusted to correspond to the MJO time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded ✓ 1.9908672098536044 seconds\n",
      "Created coordinate dictionaries ✓ 14.669434004928917 seconds\n",
      "CTRL data loaded ✓ 19.015067694010213 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "## Select Parent Directory:\n",
    "# times = [np.datetime64('2015-12-10T01'), np.datetime64('2015-12-10T02')]  # Used to debug\n",
    "\n",
    "times = [np.datetime64('2015-11-23T01'), np.datetime64('2015-12-02T00')]\n",
    "parent_dir_CTRL = '/ourdisk/hpc/radclouds/auto_archive_notyet/tape_2copies/hragnajarian/wrfout.files/10day-2015-11-22-12--12-03-00'\n",
    "parent_dir_adjLH = '/ourdisk/hpc/radclouds/auto_archive_notyet/tape_2copies/hragnajarian/wrfout.files/10day-2015-11-22-12--12-03-00/adjLH'\n",
    "\n",
    "# times = [np.datetime64('2015-12-10T01'), np.datetime64('2015-12-20T00')]\n",
    "# parent_dir_CTRL = '/ourdisk/hpc/radclouds/auto_archive_notyet/tape_2copies/hragnajarian/wrfout.files/10day-2015-12-09-12--12-20-00'\n",
    "# parent_dir_adjLH = '/ourdisk/hpc/radclouds/auto_archive_notyet/tape_2copies/hragnajarian/wrfout.files/10day-2015-12-09-12--12-20-00/adjLH'\n",
    "\n",
    "\n",
    "## Create Dictionary with file names, units, description.\n",
    "\n",
    "    # 2-D data\n",
    "l1_files = {\n",
    "    'd02_LowCLDFRA':     ('0-1',      'CLDFRA',   'Low-level cloud fraction'),\n",
    "    'd02_MidCLDFRA':     ('0-1',      'CLDFRA',   'Mid-level cloud fraction'),\n",
    "    'd02_HighCLDFRA':    ('0-1',      'CLDFRA',   'High-level cloud fraction'),\n",
    "    'd02_HFX':           ('W/m^2',    'HFX',      'Surface sensible heat flux'),\n",
    "    'd02_LH':            ('W/m^2',    'LH',       'Surface latent heat flux'),\n",
    "    'd02_SMOIS':         ('kg^3/kg^3','SMOIS',    'Soil moisture'),\n",
    "    'd02_RR':            ('mm/dt',    'RR',       'Rain rate'),\n",
    "    'd02_U10':           ('m/s',      'U10',      '10-meter U-wind'),\n",
    "    'd02_V10':           ('m/s',      'V10',      '10-meter V-wind'),\n",
    "\t'd02_SSTSK':         ('K',        'SSTSK',    'Sea surface temperature'),\n",
    "    'd02_T2':            ('K',        'T2',       '2-meter temperature'),\n",
    "    'd02_TSK':           ('K',        'TSK',      'Surface skin temperature'),\n",
    "    'd02_PSFC':          ('hPa',      'PSFC',     'Surface pressure'),\n",
    "    'd02_Q2':            ('kg/kg',    'Q2',       '2-meter specific humidity'),\n",
    "    'd02_VEGFRA':        ('0-1',      'VEGFRA',   'Vegetation fraction'),\n",
    "    'd02_CAPE':          ('J/kg',     'CAPE',     'Convective Available Potential Energy'),\n",
    "    'd02_CIN':           ('J/kg',     'CIN',      'Convective Inhibition'),\n",
    "    'd02_LWDNB':         ('W/m^2',    'LWDNB',    'SFC Downward longwave radiation'),\n",
    "    'd02_LWUPB':         ('W/m^2',    'LWUPB',    'SFC Upward longwave radiation'),\n",
    "    'd02_LWDNBC':        ('W/m^2',    'LWDNBC',   'SFC Clear-sky downward longwave radiation'),\n",
    "    'd02_LWUPBC':        ('W/m^2',    'LWUPBC',   'SFC Clear-sky upward longwave radiation'),\n",
    "    'd02_SWDNB':         ('W/m^2',    'SWDNB',    'SFC Downward shortwave radiation'),\n",
    "    'd02_SWUPB':         ('W/m^2',    'SWUPB',    'SFC Upward shortwave radiation'),\n",
    "    'd02_SWDNBC':        ('W/m^2',    'SWDNBC',   'SFC Clear-sky downward SW radiation'),\n",
    "    'd02_SWUPBC':        ('W/m^2',    'SWUPBC',   'SFC Clear-sky upward SW radiation'),\n",
    "    'd02_LWDNT':         ('W/m^2',    'LWDNT',    'TOA Downward longwave radiation'),\n",
    "    'd02_LWUPT':         ('W/m^2',    'LWUPT',    'TOA Upward longwave radiation'),\n",
    "    'd02_LWDNTC':        ('W/m^2',    'LWDNTC',   'TOA Clear-sky downward longwave radiation'),\n",
    "    'd02_LWUPTC':        ('W/m^2',    'LWUPTC',   'TOA Clear-sky upward longwave radiation'),\n",
    "    'd02_SWDNT':         ('W/m^2',    'SWDNT',    'TOA Downward shortwave radiation'),\n",
    "    'd02_SWUPT':         ('W/m^2',    'SWUPT',    'TOA Upward shortwave radiation'),\n",
    "    'd02_SWDNTC':        ('W/m^2',    'SWDNTC',   'TOA Clear-sky downward SW radiation'),\n",
    "    'd02_SWUPTC':        ('W/m^2',    'SWUPTC',   'TOA Clear-sky upward SW radiation'),\n",
    "\t# Differences in rad\n",
    "    'd02_LWSFC':         ('W/m^2',    'LWSFC',        'LWDNB - LWUPB'),\n",
    "    'd02_LWSFC_Clear':   ('W/m^2',    'LWSFC_Clear',  'LWDNBC - LWUPBC'),\n",
    "    'd02_SWSFC':         ('W/m^2',    'SWSFC',        'SWDNB - SWUPB'),\n",
    "    'd02_SWSFC_Clear':   ('W/m^2',    'SWSFC_Clear',  'SWDNBC - SWUPBC'),\n",
    "\t\n",
    "\t'd02_LWATM':         ('W/m^2',    '__xarray_dataarray_variable__',        '(LWDNT-LWUPT) + (LWUPB-LWDNB)'),\n",
    "    'd02_LWATM_Clear':   ('W/m^2',    '__xarray_dataarray_variable__',  '(LWDNTC-LWUPTC) + (LWUPBC-LWDNBC)'),\n",
    "    'd02_LWATM_CRF':     ('W/m^2',    '__xarray_dataarray_variable__',    'LWATM-LWATM_Clear'),\n",
    "\t\n",
    "    'd02_SWATM':         ('W/m^2',    '__xarray_dataarray_variable__',        '(SWDNT-SWUPT) + (SWUPB-SWDNB)'),\n",
    "    'd02_SWATM_Clear':   ('W/m^2',    '__xarray_dataarray_variable__',  '(SWDNTC-SWUPTC) + (SWUPBC-SWDNBC)'),\n",
    "    'd02_SWATM_CRF':     ('W/m^2',    '__xarray_dataarray_variable__',    'SWATM-SWATM_Clear'),\n",
    "}\n",
    "    # Interpolated 3-D data \n",
    "l2_files = {\n",
    "    'd02_interp_U':         ('m/s',     'U',         'Interpolated U-wind'),\n",
    "    'd02_interp_V':         ('m/s',     'V',         'Interpolated V-wind'),\n",
    "    'd02_interp_W':         ('m/s',     'W',         'Interpolated W-wind'),\n",
    "    'd02_interp_QV':        ('kg/kg',   'QV',        'Interpolated specific humidity'),\n",
    "    'd02_interp_CLDFRA':    ('0-1',     'CLDFRA',    'Interpolated cloud fraction'),\n",
    "    'd02_interp_H_DIABATIC':('K/s',     'H_DIABATIC','Interpolated diabatic heating'),\n",
    "    'd02_interp_Theta':     ('K',       'Theta',     'Interpolated potential temperature'),\n",
    "    'd02_interp_LWAll':     ('K/s',     'LWAll',     'Interpolated longwave heating (all-sky)'),\n",
    "    'd02_interp_LWClear':   ('K/s',     'LWClear',   'Interpolated longwave heating (clear-sky)'),\n",
    "    'd02_interp_SWAll':     ('K/s',     'SWAll',     'Interpolated shortwave heating (all-sky)'),\n",
    "    'd02_interp_SWClear':   ('K/s',     'SWClear',   'Interpolated shortwave heating (clear-sky)'),\n",
    "}\n",
    "    # Vertically integrated 3-D data\n",
    "l4_files = {\n",
    "    'd02_VI_W_1000-100':             ('kg/ms',     'W',         'Vertically Integrated Vertical Winds'),\n",
    "    'd02_VI_W_1000-500':             ('kg/ms',     'W',         'Vertically Integrated Vertical Winds'),\n",
    "    'd02_VI_W_1000-700':             ('kg/ms',     'W',         'Vertically Integrated Vertical Winds'),\n",
    "    'd02_VI_W_500-200':              ('kg/ms',     'W',         'Vertically Integrated Vertical Winds'),\n",
    "    'd02_VI_QV_1000-100':            ('kg/m^2',    'QV',        'Vertically Integrated Water Vapor'),\n",
    "    'd02_VI_QV_1000-500':            ('kg/m^2',    'QV',        'Vertically Integrated Water Vapor'),\n",
    "    'd02_VI_QV_1000-700':            ('kg/m^2',    'QV',        'Vertically Integrated Water Vapor'),\n",
    "    'd02_VI_QV_1000-800':            ('kg/m^2',    'QV',        'Vertically Integrated Water Vapor')\n",
    "}\n",
    "# Build structured dictionary\n",
    "files = {\n",
    "    'L1': {\n",
    "        var: {\n",
    "            \"path\": build_path(parent_dir_CTRL, 'L1', var),\n",
    "            \"unit\": unit,\n",
    "\t\t\t\"varname\": varname,\n",
    "            \"description\": desc\n",
    "        }\n",
    "        for var, (unit, varname, desc) in l1_files.items()\n",
    "    },\n",
    "    'L2': {\n",
    "        var: {\n",
    "            \"path\": build_path(parent_dir_CTRL, 'L2', var),\n",
    "            \"unit\": unit,\n",
    "            \"varname\": varname,\n",
    "            \"description\": desc\n",
    "        }\n",
    "        for var, (unit, varname, desc) in l2_files.items()\n",
    "    },\n",
    "    'L4': {\n",
    "        var: {\n",
    "            \"path\": build_path(parent_dir_CTRL, 'L4', var),\n",
    "            \"unit\": unit,\n",
    "\t\t\t\"varname\": varname,\n",
    "            \"description\": desc\n",
    "        }\n",
    "        for var, (unit, varname, desc) in l4_files.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "################ Declare the bounds you want to specifically look at #################\n",
    "######################################################################################\n",
    "\n",
    "\n",
    "lats = [-10, 10]\n",
    "lons = [80, 135]\n",
    "interp_P_levels = np.concatenate((np.arange(1000,950,-10),np.arange(950,350,-30),np.arange(350,0,-50)))\n",
    "\n",
    "######################################################################################\n",
    "######################################################################################\n",
    "######################################################################################\n",
    "\n",
    "# Setup the indicies that will be used throughout\n",
    "time_ind_d01, lat_ind_d01, lon_ind_d01 = isel_ind(build_path(parent_dir_CTRL, \"raw\", \"d01\"), times, lats, lons)\n",
    "time_ind_d02, lat_ind_d02, lon_ind_d02 = isel_ind(build_path(parent_dir_CTRL, \"raw\", \"d02\"), times, lats, lons)\n",
    "\n",
    "## Raw datasets\n",
    "ds_d01 = open_ds(build_path(parent_dir_CTRL, \"raw\", \"d01\"),time_ind_d01,lat_ind_d01,lon_ind_d01)\n",
    "ds_d02 = open_ds(build_path(parent_dir_CTRL, \"raw\", \"d02\"),time_ind_d02,lat_ind_d02,lon_ind_d02)\n",
    "step1_time = time.perf_counter()\n",
    "print('Dataset loaded \\N{check mark}', step1_time-start_time, 'seconds')\n",
    "\n",
    "# Coordinate dictionaries:\n",
    "step2_time = time.perf_counter()\n",
    "\n",
    "# d01_coords = dict(\n",
    "#     XLAT=(('Time','south_north','west_east'),ds_d01.XLAT.values),\n",
    "#     XLONG=(('Time','south_north','west_east'),ds_d01.XLONG.values),\n",
    "#     bottom_top=(('bottom_top'),interp_P_levels),\n",
    "#     Time=('Time',ds_d01.XTIME.values),\n",
    "#     south_north=(('south_north'),ds_d01.XLAT[0,:,0].values),\n",
    "#     west_east=(('west_east'),ds_d01.XLONG[0,0,:].values)\n",
    "#     )\n",
    "\n",
    "file_path_coords = build_path(parent_dir_CTRL,'raw','d02_coords')\n",
    "file_path_static_coords = build_path(parent_dir_CTRL,'raw','d02_static_coords')\n",
    "if file_path_coords not in glob.glob(os.path.dirname(file_path_coords) + '/*'):\n",
    "    d02_coords = dict(\n",
    "        XLAT=(('Time','south_north','west_east'),ds_d02.XLAT.values),\n",
    "        XLONG=(('Time','south_north','west_east'),ds_d02.XLONG.values),\n",
    "        bottom_top=(('bottom_top'),interp_P_levels),\n",
    "        Time=('Time',ds_d02.XTIME.values),\n",
    "        south_north=(('south_north'),ds_d02.XLAT[0,:,0].values),\n",
    "        west_east=(('west_east'),ds_d02.XLONG[0,0,:].values)\n",
    "        )\n",
    "    ## No time or bottom_top dimensions\n",
    "    d02_static_coords = dict(\n",
    "        XLAT=(('south_north','west_east'),ds_d02.XLAT[0].values),\n",
    "        XLONG=(('south_north','west_east'),ds_d02.XLONG[0].values),\n",
    "        south_north=(('south_north'),ds_d02.XLAT[0,:,0].values),\n",
    "        west_east=(('west_east'),ds_d02.XLONG[0,0,:].values)\n",
    "        )\n",
    "    ## Save coordinates so you can just load them in\n",
    "    ds_coords = xr.Dataset(coords=d02_coords)\n",
    "    ds_coords.to_netcdf(file_path_coords)\n",
    "    ds_static_coords = xr.Dataset(coords=d02_static_coords)\n",
    "    ds_static_coords.to_netcdf(file_path_static_coords)\n",
    "## Open coord files\n",
    "ds_coords = xr.open_dataset(file_path_coords)\n",
    "d02_coords = {\n",
    "    var: (ds_coords[var].dims, ds_coords[var].values)\n",
    "    for var in ds_coords.coords}\n",
    "ds_static_coords = xr.open_dataset(file_path_static_coords)\n",
    "d02_static_coords = {\n",
    "    var: (ds_static_coords[var].dims, ds_static_coords[var].values)\n",
    "    for var in ds_static_coords.coords}\n",
    "\n",
    "step1_time = time.perf_counter()\n",
    "print('Created coordinate dictionaries \\N{check mark}', step1_time-step2_time, 'seconds')\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "################# Load in the variables ###################\n",
    "##################### 3-D variables #######################\n",
    "# da_d02_U = load_variable(files['L2']['d02_interp_U'], d02_coords, False)\n",
    "# da_d02_V = load_variable(files['L2']['d02_interp_V'], d02_coords, False)\n",
    "# da_d02_W = load_variable(files['L2']['d02_interp_W'], d02_coords, False)\n",
    "# da_d02_QV = load_variable(files['L2']['d02_interp_QV'], d02_coords, False)\n",
    "# da_d02_CLDFRA = load_variable(files['L2']['d02_interp_CLDFRA'], d02_coords, False)\n",
    "# da_d02_H_DIABATIC = load_variable(files['L2']['d02_interp_H_DIABATIC'], d02_coords, False)\n",
    "# da_d02_Theta = load_variable(files['L2']['d02_interp_Theta'], d02_coords, False)\n",
    "# da_d02_LWAll = load_variable(files['L2']['d02_interp_LWAll'], d02_coords, False)\n",
    "# da_d02_LWClear = load_variable(files['L2']['d02_interp_LWClear'], d02_coords, False)\n",
    "# da_d02_SWAll = load_variable(files['L2']['d02_interp_SWAll'], d02_coords, False)\n",
    "# da_d02_SWClear = load_variable(files['L2']['d02_interp_SWClear'], d02_coords, False)\n",
    "# ##Cloud-Radiative Forcing Calculations ##\n",
    "# da_d02_LWCRF = da_d02_LWAll - da_d02_LWClear\t# Longwave\n",
    "# da_d02_SWCRF = da_d02_SWAll - da_d02_SWClear\t# Shortwave\n",
    "# da_d02_TotalCRF = da_d02_LWCRF + da_d02_SWCRF\t# Total\n",
    "\n",
    "###########################################################\n",
    "##################### 2-D variables #######################\n",
    "# da_d02_RR = load_variable(files['L1']['d02_RR'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_LowCLDFRA = load_variable(files['L1']['d02_LowCLDFRA'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_MidCLDFRA = load_variable(files['L1']['d02_MidCLDFRA'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_HighCLDFRA = load_variable(files['L1']['d02_HighCLDFRA'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_HFX = load_variable(files['L1']['d02_HFX'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_LH = load_variable(files['L1']['d02_LH'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_U10 = load_variable(files['L1']['d02_U10'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_V10 = load_variable(files['L1']['d02_V10'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_SMOIS = load_variable(files['L1']['d02_SMOIS'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_VEGFRA = load_variable(files['L1']['d02_VEGFRA'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_SSTSK = load_variable(files['L1']['d02_SSTSK'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_T2 = load_variable(files['L1']['d02_T2'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_TSK = load_variable(files['L1']['d02_TSK'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_PSFC = load_variable(files['L1']['d02_PSFC'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_Q2 = load_variable(files['L1']['d02_Q2'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_CAPE = load_variable(files['L1']['d02_CAPE'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_CIN = load_variable(files['L1']['d02_CIN'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# ## Extra Variable Calculations\n",
    "# da_d02_QS2 = sat_mixing_ratio(sat_vap_press(da_d02_T2),da_d02_PSFC)\n",
    "# da_d02_RH2 = rel_hum_temp(da_d02_T2, da_d02_Q2, da_d02_PSFC)\t# Calculates relative humidty (%) using temperature and q at surface\n",
    "# da_d02_Uwind10 = (da_d02_U10**2+da_d02_V10**2)**.5\n",
    "# da_d02_Udir10 = wind_direction(da_d02_U10, da_d02_V10)\n",
    "\n",
    "## Vertically Integrated\n",
    "    # Vertical Wind\n",
    "    # Add negative to W so upward motion is positive\n",
    "# da_d02_VI_W_1000_100 = -load_variable(files['L4']['d02_VI_W_1000-100'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_VI_W_1000_500 = -load_variable(files['L4']['d02_VI_W_1000-500'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_VI_W_1000_700 = -load_variable(files['L4']['d02_VI_W_1000-700'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_VI_W_500_200 = -load_variable(files['L4']['d02_VI_W_500-200'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "#     # Water Vapor\n",
    "# da_d02_VI_QV_1000_100 = -load_variable(files['L4']['d02_VI_QV_1000-100'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_VI_QV_1000_500 = -load_variable(files['L4']['d02_VI_QV_1000-500'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_VI_QV_1000_700 = -load_variable(files['L4']['d02_VI_QV_1000-700'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_VI_QV_1000_800 = -load_variable(files['L4']['d02_VI_QV_1000-800'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "\n",
    "## Surface\n",
    "# da_d02_LWSFC = diff_load_or_save({'path':build_path(parent_dir_CTRL, 'L1', 'd02_LWSFC'), 'varname':'LWSFC', 'description':'LWDNB - LWUPB'},\n",
    "# \t\t\t\t   files['L1']['d02_LWDNB'], files['L1']['d02_LWUPB'], without_keys(d02_coords, 'bottom_top'), file_1_and_2_diff=False)\n",
    "# da_d02_LWSFC_Clear = diff_load_or_save({'path':build_path(parent_dir_CTRL, 'L1', 'd02_LWSFC_Clear'), 'varname':'LWSFC_Clear', 'description':'LWDNBC - LWUPBC'},\n",
    "# \t\t\t\t   files['L1']['d02_LWDNBC'], files['L1']['d02_LWUPBC'], without_keys(d02_coords, 'bottom_top'), file_1_and_2_diff=False)\n",
    "# da_d02_LWSFC_CRF = diff_load_or_save({'path':build_path(parent_dir_CTRL, 'L1', 'd02_LWSFC_CRF'), 'varname':'LWSFC_CRF', 'description':'LWSFC - LWSFC_Clear'},\n",
    "# \t\t\t\t   files['L1']['d02_LWSFC'], files['L1']['d02_LWSFC_Clear'], without_keys(d02_coords, 'bottom_top'), file_1_and_2_diff=True)\n",
    "\n",
    "# da_d02_SWSFC = diff_load_or_save({'path':build_path(parent_dir_CTRL, 'L1', 'd02_SWSFC'), 'varname':'SWSFC', 'description':'SWDNB - SWUPB'},\n",
    "# \t\t\t\t   files['L1']['d02_SWDNB'], files['L1']['d02_SWUPB'], without_keys(d02_coords, 'bottom_top'), file_1_and_2_diff=False)\n",
    "# da_d02_SWSFC_Clear = diff_load_or_save({'path':build_path(parent_dir_CTRL, 'L1', 'd02_SWSFC_Clear'), 'varname':'SWSFC_Clear', 'description':'SWDNBC - SWUPBC'},\n",
    "# \t\t\t\t   files['L1']['d02_SWDNBC'], files['L1']['d02_SWUPBC'], without_keys(d02_coords, 'bottom_top'), file_1_and_2_diff=False)\n",
    "# da_d02_SWSFC_CRF = diff_load_or_save({'path':build_path(parent_dir_CTRL, 'L1', 'd02_SWSFC_CRF'), 'varname':'SWSFC_CRF', 'description':'SWSFC - SWSFC_Clear'},\n",
    "# \t\t\t\t   files['L1']['d02_SWSFC'], files['L1']['d02_SWSFC_Clear'], without_keys(d02_coords, 'bottom_top'), file_1_and_2_diff=True)\n",
    "\n",
    "da_d02_LAKEMASK = ds_d02['LAKEMASK'].sel(Time=slice(1)).compute().squeeze()     # Lake = 1, Non-lake = 0\n",
    "da_d02_LANDMASK = ds_d02['LANDMASK'].sel(Time=slice(1)).compute().squeeze()     # Land = 1, Water = 0\n",
    "# Update LANDMASK to mask lakes into land\n",
    "da_d02_LANDMASK = xr.where((da_d02_LAKEMASK==1), 1, da_d02_LANDMASK)            # If lake (1), change landmask of 0 to land (1).\n",
    "da_d02_LANDMASK = da_d02_LANDMASK.assign_coords(d02_static_coords)\n",
    "\n",
    "da_d02_TOPO = ds_d02['HGT'].sel(Time=slice(1)).compute().squeeze().assign_coords(d02_static_coords)\n",
    "\n",
    "# x = da_d02_LANDMASK.copy().isel(south_north=slice(int((abs(da_d02_LANDMASK.XLAT[:,0]+10)).argmin()),int((abs(da_d02_LANDMASK.XLAT[:,0]-5)).argmin())))\n",
    "# print(f'Land/Ocean Ratio: {((x==1).sum()/(x==0).sum()).values}')\n",
    "\n",
    "dx_km = 3\n",
    "min_cluster_size = 600\t\t# 1500 for only large islands, 100 for getting rid of small islands, 600 for getting rid of medium islands\n",
    "da_distance_from_coast, dist_bins = compute_coast_distance(da_d02_LANDMASK, min_cluster_size, dx_km)\n",
    "\n",
    "print('CTRL data loaded \\N{check mark}', time.perf_counter()-start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted LH Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded ✓ 1.474689835915342 seconds\n",
      "Created coordinate dictionaries ✓ 14.341381998034194 seconds\n",
      "NCRF data loaded ✓ 15.816352315014228 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "# Create new dictionary with 'sunrise' inserted into variable names\n",
    "files_sunrise = {\n",
    "    level: {\n",
    "        var.replace('d02_', 'd02_sunrise_'): {\n",
    "            \"path\": build_path(parent_dir_NCRF, level, var.replace('d02_', 'd02_sunrise_')),\n",
    "            \"unit\": props[\"unit\"],\n",
    "            \"varname\": props[\"varname\"],\n",
    "            \"description\": props[\"description\"]\n",
    "        }\n",
    "        for var, props in level_dict.items()\n",
    "        if var.startswith('d02_')  # Only modify variables that start with 'd02_'\n",
    "    }\n",
    "    for level, level_dict in files.items()\n",
    "}\n",
    "\n",
    "## Setup the indicies that will be used throughout\n",
    "time_ind_d02, lat_ind_d02, lon_ind_d02 = isel_ind(build_path(parent_dir_NCRF, \"raw\", \"d02_sunrise\"), times, lats, lons)\n",
    "\n",
    "## Raw datasets\n",
    "ds_d02_sunrise = open_ds(build_path(parent_dir_NCRF, \"raw\", \"d02_sunrise\"),time_ind_d02,lat_ind_d02,lon_ind_d02)\n",
    "step1_time = time.perf_counter()\n",
    "print('Dataset loaded \\N{check mark}', step1_time-start_time, 'seconds')\n",
    "\n",
    "# Coordinate dictionaries:\n",
    "step2_time = time.perf_counter()\n",
    "\n",
    "file_path_coords = build_path(parent_dir_NCRF,'raw','d02_coords')\n",
    "file_path_static_coords = build_path(parent_dir_NCRF,'raw','d02_static_coords')\n",
    "if file_path_coords not in glob.glob(os.path.dirname(file_path_coords) + '/*'):\n",
    "    d02_coords = dict(\n",
    "        XLAT=(('Time','south_north','west_east'),ds_d02.XLAT.values),\n",
    "        XLONG=(('Time','south_north','west_east'),ds_d02.XLONG.values),\n",
    "        bottom_top=(('bottom_top'),interp_P_levels),\n",
    "        Time=('Time',ds_d02.XTIME.values),\n",
    "        south_north=(('south_north'),ds_d02.XLAT[0,:,0].values),\n",
    "        west_east=(('west_east'),ds_d02.XLONG[0,0,:].values)\n",
    "        )\n",
    "    ## No time or bottom_top dimensions\n",
    "    d02_static_coords = dict(\n",
    "        XLAT=(('south_north','west_east'),ds_d02.XLAT[0].values),\n",
    "        XLONG=(('south_north','west_east'),ds_d02.XLONG[0].values),\n",
    "        south_north=(('south_north'),ds_d02.XLAT[0,:,0].values),\n",
    "        west_east=(('west_east'),ds_d02.XLONG[0,0,:].values)\n",
    "        )\n",
    "    ## Save coordinates so you can just load them in\n",
    "    ds_coords = xr.Dataset(coords=d02_coords)\n",
    "    ds_coords.to_netcdf(file_path_coords)\n",
    "    ds_static_coords = xr.Dataset(coords=d02_static_coords)\n",
    "    ds_static_coords.to_netcdf(file_path_static_coords)\n",
    "## Open coord files\n",
    "ds_coords = xr.open_dataset(file_path_coords)\n",
    "d02_coords = {\n",
    "    var: (ds_coords[var].dims, ds_coords[var].values)\n",
    "    for var in ds_coords.coords}\n",
    "ds_static_coords = xr.open_dataset(file_path_static_coords)\n",
    "d02_static_coords = {\n",
    "    var: (ds_static_coords[var].dims, ds_static_coords[var].values)\n",
    "    for var in ds_static_coords.coords}\n",
    "\n",
    "step1_time = time.perf_counter()\n",
    "print('Created coordinate dictionaries \\N{check mark}', step1_time-step2_time, 'seconds')\n",
    "\n",
    "###########################################################\n",
    "################# Load in the variables ###################\n",
    "##################### 3-D variables #######################\n",
    "\n",
    "# da_d02_sunrise_U = load_variable(files_sunrise['L2']['d02_sunrise_interp_U'], d02_coords, False)\n",
    "# da_d02_sunrise_V = load_variable(files_sunrise['L2']['d02_sunrise_interp_V'], d02_coords, False)\n",
    "# da_d02_sunrise_W = load_variable(files_sunrise['L2']['d02_sunrise_interp_W'], d02_coords, False)\n",
    "# da_d02_sunrise_QV = load_variable(files_sunrise['L2']['d02_sunrise_interp_QV'], d02_coords, False)\n",
    "# da_d02_sunrise_CLDFRA = load_variable(files_sunrise['L2']['d02_sunrise_interp_CLDFRA'], d02_coords, False)\n",
    "# da_d02_sunrise_H_DIABATIC = load_variable(files_sunrise['L2']['d02_sunrise_interp_H_DIABATIC'], d02_coords, False)\n",
    "# da_d02_sunrise_Theta = load_variable(files_sunrise['L2']['d02_sunrise_interp_Theta'], d02_coords, False)\n",
    "# da_d02_sunrise_LWAll = load_variable(files_sunrise['L2']['d02_sunrise_interp_LWAll'], d02_coords, False)\n",
    "# da_d02_sunrise_LWClear = load_variable(files_sunrise['L2']['d02_sunrise_interp_LWClear'], d02_coords, False)\n",
    "# da_d02_sunrise_SWAll = load_variable(files_sunrise['L2']['d02_sunrise_interp_SWAll'], d02_coords, False)\n",
    "# da_d02_sunrise_SWClear = load_variable(files_sunrise['L2']['d02_sunrise_interp_SWClear'], d02_coords, False)\n",
    "# ##Cloud-Radiative Forcing Calculations ##\n",
    "# da_d02_sunrise_LWCRF = da_d02_sunrise_LWAll - da_d02_sunrise_LWClear\t# Longwave\n",
    "# da_d02_sunrise_SWCRF = da_d02_sunrise_SWAll - da_d02_sunrise_SWClear\t# Shortwave\n",
    "# da_d02_sunrise_TotalCRF = da_d02_sunrise_LWCRF + da_d02_sunrise_SWCRF\t# Total\n",
    "\n",
    "###########################################################\n",
    "##################### 2-D variables #######################\n",
    "\n",
    "# da_d02_sunrise_RR = load_variable(files_sunrise['L1']['d02_sunrise_RR'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_HFX = load_variable(files_sunrise['L1']['d02_sunrise_HFX'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_LH = load_variable(files_sunrise['L1']['d02_sunrise_LH'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_U10 = load_variable(files_sunrise['L1']['d02_sunrise_U10'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_V10 = load_variable(files_sunrise['L1']['d02_sunrise_V10'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_SMOIS = load_variable(files_sunrise['L1']['d02_sunrise_SMOIS'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_VEGFRA = load_variable(files_sunrise['L1']['d02_sunrise_VEGFRA'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_SSTSK = load_variable(files_sunrise['L1']['d02_sunrise_SSTSK'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_T2 = load_variable(files_sunrise['L1']['d02_sunrise_T2'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_TSK = load_variable(files_sunrise['L1']['d02_sunrise_TSK'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_PSFC = load_variable(files_sunrise['L1']['d02_sunrise_PSFC'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_Q2 = load_variable(files_sunrise['L1']['d02_sunrise_Q2'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_CAPE = load_variable(files_sunrise['L1']['d02_sunrise_CAPE'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_CIN = load_variable(files_sunrise['L1']['d02_sunrise_CIN'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# ## Extra Variable Calculations\n",
    "# da_d02_sunrise_QS2 = sat_mixing_ratio(sat_vap_press(da_d02_sunrise_T2),da_d02_sunrise_PSFC)\n",
    "# da_d02_sunrise_RH2 = rel_hum_temp(da_d02_sunrise_T2, da_d02_sunrise_Q2, da_d02_sunrise_PSFC)\t# Calculates relative humidty (%) using temperature and q at surface\n",
    "# da_d02_sunrise_Uwind10 = (da_d02_sunrise_U10**2+da_d02_sunrise_V10**2)**.5\n",
    "# da_d02_sunrise_Udir10 = wind_direction(da_d02_sunrise_U10, da_d02_sunrise_V10)\n",
    "\n",
    "## Vertically Integrated\n",
    "    # Vertical Wind\n",
    "    # Add negative to W so upward motion is positive\n",
    "# da_d02_sunrise_VI_W_1000_100 = -load_variable(files_sunrise['L4']['d02_sunrise_VI_W_1000-100'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_VI_W_1000_500 = -load_variable(files_sunrise['L4']['d02_sunrise_VI_W_1000-500'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_VI_W_1000_700 = -load_variable(files_sunrise['L4']['d02_sunrise_VI_W_1000-700'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_VI_W_500_200 = -load_variable(files_sunrise['L4']['d02_sunrise_VI_W_500-200'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "    # Water Vapor\n",
    "# da_d02_sunrise_VI_QV_1000_100 = -load_variable(files_sunrise['L4']['d02_sunrise_VI_QV_1000-100'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_VI_QV_1000_500 = -load_variable(files_sunrise['L4']['d02_sunrise_VI_QV_1000-500'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_VI_QV_1000_700 = -load_variable(files_sunrise['L4']['d02_sunrise_VI_QV_1000-700'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "# da_d02_sunrise_VI_QV_1000_800 = -load_variable(files_sunrise['L4']['d02_sunrise_VI_QV_1000-800'], without_keys(d02_coords, 'bottom_top'), False)\n",
    "\n",
    "# da_d02_sunrise_LWSFC = diff_load_or_save({'path':build_path(parent_dir_NCRF, 'L1', 'd02_sunrise_LWSFC'), 'varname':'LWSFC', 'description':'LWDNB - LWUPB'},\n",
    "# \t\t\t\t   files_sunrise['L1']['d02_sunrise_LWDNB'], files_sunrise['L1']['d02_sunrise_LWUPB'], without_keys(d02_coords, 'bottom_top'), file_1_and_2_diff=False)\n",
    "# da_d02_sunrise_SWSFC = diff_load_or_save({'path':build_path(parent_dir_NCRF, 'L1', 'd02_sunrise_SWSFC'), 'varname':'SWSFC', 'description':'SWDNB - SWUPB'},\n",
    "# \t\t\t\t   files_sunrise['L1']['d02_sunrise_SWDNB'], files_sunrise['L1']['d02_sunrise_SWUPB'], without_keys(d02_coords, 'bottom_top'), file_1_and_2_diff=False)\n",
    "\n",
    "print('NCRF data loaded \\N{check mark}', time.perf_counter()-start_time, 'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WRF_Xarray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
